{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 60  # Number of frames; adjust based on your data\n",
    "slide_step = 20  # Adjust for finer or coarser granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RdmDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        classes = {'GOUP': 0, 'BLNC': 1, 'DOWN': 2}\n",
    "        \n",
    "        folders = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        split_idx = int(len(folders) * 0.8)  # 80-20 split for train-test\n",
    "        selected_folders = folders[:split_idx] if train else folders[split_idx:]\n",
    "        print(f\"Is_train? {train}\")\n",
    "        print(selected_folders)\n",
    "        \n",
    "        for folder in selected_folders:\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith('.npy'):\n",
    "                    class_label = [classes[key] for key in classes if key in file.upper()][0]\n",
    "                    rdm_data = np.load(os.path.join(folder_path, file))\n",
    "                    \n",
    "                    if rdm_data.shape[1] != 23 or rdm_data.shape[2] != 13:\n",
    "                        print(rdm_data.shape)\n",
    "                        break\n",
    "                    \n",
    "                    self.data.append(torch.tensor(rdm_data, dtype=torch.float).unsqueeze(0))  # Add channel dimension\n",
    "                    self.labels.append(class_label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assume all frames within a sequence are of the same shape, which is a prerequisite.\n",
    "        # If they are not, you need to resize them or handle it as needed.\n",
    "        rdm_sequence = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # If resizing is necessary (e.g., if frames in the same sequence could have different shapes),\n",
    "        # you can use torchvision.transforms.Resize:\n",
    "        # resize_transform = Resize(size=(desired_height, desired_width))\n",
    "        # rdm_sequence = torch.stack([resize_transform(frame) for frame in rdm_sequence])\n",
    "\n",
    "        # Padding the sequence to the max length will be done in the collate_fn\n",
    "    \n",
    "        return rdm_sequence, self.labels[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    # Separate the sequences and labels\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Remove the extra leading dimension and calculate lengths for each sequence in the batch\n",
    "    # The extra dimension is removed by using .squeeze(0), which removes the dimension of size 1 at position 0\n",
    "    sequences = [seq.squeeze(0) for seq in sequences]  # Adjusted sequences are now [frames, 23, 13]\n",
    "    lengths = [seq.size(0) for seq in sequences]  # Now seq is [frames, 23, 13]\n",
    "    \n",
    "    # Pad sequences to have the same length\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
    "    \n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return sequences_padded, labels, lengths\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class RdmClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size):\n",
    "        super(RdmClassifier, self).__init__()\n",
    "        # Define a simple CNN architecture\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # Assuming RDMs have a single channel\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),  # This will flatten the output of the convolutional layers\n",
    "        )\n",
    "        cnn_output_size = self._get_conv_output_size()\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(cnn_output_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Define the fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len, _, _ = x.size()\n",
    "        # Apply CNN to each RDM in the sequence\n",
    "        c_out = self.cnn(x.view(batch_size * seq_len, 1, *x.size()[-2:]))\n",
    "        \n",
    "        # Reshape for LSTM input\n",
    "        r_out = c_out.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Pack the sequence for LSTM\n",
    "        packed_input = pack_padded_sequence(r_out, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, _) = self.lstm(packed_input)\n",
    "\n",
    "        # Use the last hidden state\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out\n",
    "    \n",
    "    def _get_conv_output_size(self):\n",
    "        # We can create a dummy input to calculate the output size after the CNN layers\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 23, 13)  # Replace with your RDM shape\n",
    "            dummy_output = self.cnn(dummy_input)\n",
    "            return dummy_output.size(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is_train? True\n",
      "['10', '14', '08', '04', '12', '24', '02', '05', '18', '01', '16', '22']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/var/folders/nh/xjdmv90x37b3xwwbwbm5lp1w0000gn/T/ipykernel_86506/2079292126.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(outputs, torch.tensor(labels, dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.071485996246338\n",
      "Epoch [2/5], Loss: 0.9400832653045654\n",
      "Epoch [3/5], Loss: 0.04560468718409538\n",
      "Epoch [4/5], Loss: 0.07244273275136948\n",
      "Epoch [5/5], Loss: 0.047632619738578796\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "root_dir = '/Volumes/FourTBLaCie/Yoga_Study_RADAR_1CH_GOUP_BLNC_DOWN'  # Replace with the correct root directory\n",
    "train_dataset = RdmDataset(root_dir, train=True)\n",
    "\n",
    "# Update your DataLoader to use the new collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 3  # Each frame is a 2D Array\n",
    "hidden_size = 128\n",
    "num_classes = 3\n",
    "\n",
    "# Instantiate the model\n",
    "model = RdmClassifier(num_classes=3, hidden_size=hidden_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for padded_rdm, labels, lengths in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(padded_rdm, lengths)\n",
    "            loss = criterion(outputs, torch.tensor(labels, dtype=torch.long))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "        \n",
    "\n",
    "# Complete the evaluate_model function\n",
    "\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5  # Define the number of epochs\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_and_predict(model, data, window_size, slide_step):\n",
    "    predictions = []\n",
    "    for start in range(0, len(data) - window_size + 1, slide_step):\n",
    "        end = start + window_size\n",
    "        window_data =  data[start:end]\n",
    "        prediction = model.predict(window_data)  # Adjust based on your model's prediction method\n",
    "        predictions.append((start, end, prediction))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, segments, labels):\n",
    "        self.segments = segments  # Your preprocessed and segmented RADAR data\n",
    "        self.labels = labels  # Corresponding labels for each segment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.segments[idx], self.labels[idx]\n",
    "\n",
    "# Example usage\n",
    "dataset = RadarDataset(segments, labels)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_predictions(predictions, event_labels):\n",
    "    events_detected = []\n",
    "    for start, end, prediction in predictions:\n",
    "        if prediction == 'GOUP' and any((label['Start_Frame'] <= start < label['frame_stable']) or\n",
    "                                        (label['Start_Frame'] < end <= label['frame_stable']) for _, label in event_labels.iterrows()):\n",
    "            events_detected.append((start, end, 'GOUP'))\n",
    "        elif prediction == 'DOWN' and any((label['frame_break'] <= start < label['End_Frame']) or\n",
    "                                          (label['frame_break'] < end <= label['End_Frame']) for _, label in event_labels.iterrows()):\n",
    "            events_detected.append((start, end, 'DOWN'))\n",
    "    return events_detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming processed_data is loaded for each RADAR capture\n",
    "# Example for a single RADAR capture; loop or functionize for all captures as needed\n",
    "radar_capture_name = '01_MNTRL_RR_V1'\n",
    "participant = radar_capture_name[:2]\n",
    "processed_data = np.load(f'/path/to/processed/data/{radar_capture_name}.npy')  # Adjust path as necessary\n",
    "event_labels = event_labels_df[event_labels_df['RADAR_capture'] == radar_capture_name]\n",
    "\n",
    "# Parameters for sliding window\n",
    "window_size = 100  # Example size, adjust as needed\n",
    "slide_step = 10  # Example step size, adjust as needed\n",
    "\n",
    "# Predict events\n",
    "predictions = slide_and_predict(model, processed_data, window_size, slide_step, event_labels)\n",
    "events_detected = interpret_predictions(predictions, event_labels)\n",
    "\n",
    "# Do something with detected events\n",
    "for event in events_detected:\n",
    "    print(event)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('radartreepose_env_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e87b78f46a7b740412dfe5289434eafd4b7f2098f84e930a0f6749e61e586d83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
