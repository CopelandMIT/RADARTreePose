{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RdmDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        classes = {'GOUP': 0, 'BLNC': 1, 'DOWN': 2}\n",
    "        \n",
    "        folders = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        split_idx = int(len(folders) * 0.8)  # 80-20 split for train-test\n",
    "        selected_folders = folders[:split_idx] if train else folders[split_idx:]\n",
    "        print(f\"Is_train? {train}\")\n",
    "        print(selected_folders)\n",
    "        \n",
    "        for folder in selected_folders:\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith('.npy'):\n",
    "                    class_label = [classes[key] for key in classes if key in file.upper()][0]\n",
    "                    rdm_data = np.load(os.path.join(folder_path, file))\n",
    "                    \n",
    "                    if rdm_data.shape[1] != 23 or rdm_data.shape[2] != 13:\n",
    "                        print(rdm_data.shape)\n",
    "                        break\n",
    "                    \n",
    "                    self.data.append(torch.tensor(rdm_data, dtype=torch.float).unsqueeze(0))  # Add channel dimension\n",
    "                    self.labels.append(class_label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assume all frames within a sequence are of the same shape, which is a prerequisite.\n",
    "        # If they are not, you need to resize them or handle it as needed.\n",
    "        rdm_sequence = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # If resizing is necessary (e.g., if frames in the same sequence could have different shapes),\n",
    "        # you can use torchvision.transforms.Resize:\n",
    "        # resize_transform = Resize(size=(desired_height, desired_width))\n",
    "        # rdm_sequence = torch.stack([resize_transform(frame) for frame in rdm_sequence])\n",
    "\n",
    "        # Padding the sequence to the max length will be done in the collate_fn\n",
    "    \n",
    "        return rdm_sequence, self.labels[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    # Separate the sequences and labels\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Remove the extra leading dimension and calculate lengths for each sequence in the batch\n",
    "    # The extra dimension is removed by using .squeeze(0), which removes the dimension of size 1 at position 0\n",
    "    sequences = [seq.squeeze(0) for seq in sequences]  # Adjusted sequences are now [frames, 23, 13]\n",
    "    lengths = [seq.size(0) for seq in sequences]  # Now seq is [frames, 23, 13]\n",
    "    \n",
    "    # Pad sequences to have the same length\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
    "    \n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return sequences_padded, labels, lengths\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class RdmClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size):\n",
    "        super(RdmClassifier, self).__init__()\n",
    "        # Define a simple CNN architecture\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # Assuming RDMs have a single channel\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),  # This will flatten the output of the convolutional layers\n",
    "        )\n",
    "        cnn_output_size = self._get_conv_output_size()\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(cnn_output_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Define the fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len, _, _ = x.size()\n",
    "        # Apply CNN to each RDM in the sequence\n",
    "        c_out = self.cnn(x.view(batch_size * seq_len, 1, *x.size()[-2:]))\n",
    "        \n",
    "        # Reshape for LSTM input\n",
    "        r_out = c_out.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Pack the sequence for LSTM\n",
    "        packed_input = pack_padded_sequence(r_out, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, _) = self.lstm(packed_input)\n",
    "\n",
    "        # Use the last hidden state\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out\n",
    "    \n",
    "    def _get_conv_output_size(self):\n",
    "        # We can create a dummy input to calculate the output size after the CNN layers\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 23, 13)  # Replace with your RDM shape\n",
    "            dummy_output = self.cnn(dummy_input)\n",
    "            return dummy_output.size(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is_train? True\n",
      "['12', '02', '04', '24', '16', '08', '01', '18', '03', '05', '15', '14']\n",
      "Is_train? False\n",
      "['02', '03', '10']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/var/folders/nh/xjdmv90x37b3xwwbwbm5lp1w0000gn/T/ipykernel_86312/512299123.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(outputs, torch.tensor(labels, dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.1455981731414795\n",
      "Epoch [2/5], Loss: 0.8852002024650574\n",
      "Epoch [3/5], Loss: 0.7341263294219971\n",
      "Epoch [4/5], Loss: 0.04041285067796707\n",
      "Epoch [5/5], Loss: 0.11543883383274078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nh/xjdmv90x37b3xwwbwbm5lp1w0000gn/T/ipykernel_86312/512299123.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  correct += (predicted == torch.tensor(labels, dtype=torch.long)).sum().item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.1875%\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "root_dir = '/Volumes/FourTBLaCie/Yoga_Study_RADAR_1CH_GOUP_BLNC_DOWN'  # Replace with the correct root directory\n",
    "train_dataset = RdmDataset(root_dir, train=True)\n",
    "test_dataset = RdmDataset(root_dir, train=False)\n",
    "\n",
    "\n",
    "# Update your DataLoader to use the new collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 3  # Each frame is a 2D Array\n",
    "hidden_size = 128\n",
    "num_classes = 3\n",
    "\n",
    "# Instantiate the model\n",
    "model = RdmClassifier(num_classes=3, hidden_size=hidden_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for padded_rdm, labels, lengths in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(padded_rdm, lengths)\n",
    "            loss = criterion(outputs, torch.tensor(labels, dtype=torch.long))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "        \n",
    "\n",
    "# Complete the evaluate_model function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for padded_rdm, labels, lengths in test_loader:\n",
    "            outputs = model(padded_rdm, lengths)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == torch.tensor(labels, dtype=torch.long)).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5  # Define the number of epochs\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('radartreepose_env_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e87b78f46a7b740412dfe5289434eafd4b7f2098f84e930a0f6749e61e586d83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
