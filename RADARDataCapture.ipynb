{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a class for processing an FMCW RADAR data capture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from datetime import datetime\n",
    "from scipy.signal import find_peaks, correlate\n",
    "from scipy.ndimage import convolve\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMCWRADARDataCapture:\n",
    "    \"\"\"\n",
    "    Class for handling the capture, processing, and saving of FMCW RADAR data.\n",
    "\n",
    "    This class is designed to load Frequency-Modulated Continuous-Wave (FMCW) RADAR data from a specified HDF5 file,\n",
    "    process the data into a usable format, and save it as a NumPy file (either .npy or .npz format).\n",
    "\n",
    "    Attributes:\n",
    "        file_path (str): Path to the HDF5 file containing the RADAR data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initializes the FMCWRADARDataCapture class with the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the HDF5 file to be loaded and processed.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.output_path = self.file_path.replace(\"_Data\", \"_Data_NP\")\n",
    "\n",
    "    def load_and_save(self, output_path=None, format='npy', save_npy = False):\n",
    "        if output_path is None:\n",
    "            output_path = self.output_path \n",
    "            output_path = os.path.splitext(output_path)[0]\n",
    "            \n",
    "        # Ensure the directory of the output path exists\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        dataCubes = np.zeros((4, 1000, 128, 256))\n",
    "        \n",
    "\n",
    "        # Open the HDF5 file for reading\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            # Extract parameters from the file\n",
    "            FreqStrt = file['/BrdCfg/FreqStrt'][()]\n",
    "            FreqStop = file['/BrdCfg/FreqStop'][()]\n",
    "            N = 256  # Number of samples per chirp, assuming it's given or extracted correctly\n",
    "            Np = 128  # Number of chirps per frame, assuming it's given or extracted correctly\n",
    "\n",
    "            # Calculate the effective bandwidth\n",
    "            B = (FreqStop - FreqStrt) / 284 * 256\n",
    "            \n",
    "            # Read the If signal\n",
    "            If = file['/If'][:]\n",
    "            NrFrms, Nx = If.shape\n",
    "            \n",
    "            # Initialize the dataCubes array to hold the processed data for each channel\n",
    "            dataCubes = np.zeros((4, NrFrms, N, Np))\n",
    "            \n",
    "            # Process each frame\n",
    "            for frame_idx in range(NrFrms-1):\n",
    "                print(f'Processing RD Frame: {frame_idx + 1}')\n",
    "                for channel_idx in range(4):\n",
    "                    start_idx = channel_idx * N * Np\n",
    "                    end_idx = (channel_idx + 1) * N * Np\n",
    "                    # Correctly reshape and assign the data to the corresponding channel and frame\n",
    "                    reshaped_data = If[frame_idx, start_idx:end_idx].reshape(Np, N)\n",
    "                    dataCubes[channel_idx, frame_idx, :, :] = reshaped_data.transpose()  # Transpose operation\n",
    "\n",
    "            # Verify by plotting the first channel of the first frame\n",
    "            plt.imshow(dataCubes[0, 0, :, :])\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "        \n",
    "            if save_npy:\n",
    "                # Save data in the specified format\n",
    "                if format == 'npy':\n",
    "                    np.save(output_path, dataCubes)\n",
    "                elif format == 'npz':\n",
    "                    np.savez(output_path, dataCubes)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported format. Use 'npy' or 'npz'.\")\n",
    "            \n",
    "        return dataCubes\n",
    "\n",
    "    @staticmethod\n",
    "    def rawDataToDataCube(rawData, numFrames, numChirpsPerFrame, numSamplesPerChirp, numAntennas):\n",
    "        # Reshape and rearrange the rawData\n",
    "        matrixData = rawData.T.reshape(numChirpsPerFrame * numSamplesPerChirp, numFrames * numAntennas)\n",
    "        dataCubes = np.zeros((numFrames, numChirpsPerFrame, numSamplesPerChirp, numAntennas))\n",
    "\n",
    "        for frame in range(numFrames):\n",
    "            for antenna in range(numAntennas):\n",
    "                chirps = matrixData[:, frame * numAntennas + antenna]\n",
    "                chirpsMatrix = chirps.reshape(numSamplesPerChirp, numChirpsPerFrame)\n",
    "                dataCubes[frame, :, :, antenna] = chirpsMatrix.T\n",
    "                \n",
    "\n",
    "        return dataCubes.transpose((3,0,1,2))\n",
    "    \n",
    "    def range_doppler_processing(self, dataCubes):\n",
    "        \"\"\"\n",
    "        Processes each frame in the dataCube for each channel to generate Range-Doppler Maps.\n",
    "\n",
    "        Args:\n",
    "            dataCube (np.ndarray): The raw data cubes to be processed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of processed Range-Doppler Maps for each channel.\n",
    "        \"\"\"\n",
    "        n_channels, n_frames, n_bins, n_doppler = dataCubes.shape\n",
    "        rdm_all_channels = []\n",
    "\n",
    "        # Define a window function for the range and Doppler dimensions\n",
    "        range_window = np.hanning(n_bins)\n",
    "        doppler_window = np.hanning(n_doppler)\n",
    "\n",
    "        for channel_idx in range(n_channels):\n",
    "            rdm_list = []\n",
    "            for frame_idx in range(n_frames):\n",
    "                # Extract current data for the frame and channel\n",
    "                current_data = dataCubes[channel_idx, frame_idx, :, :]\n",
    "\n",
    "                # Apply the Hanning window function\n",
    "                windowed_data = np.outer(range_window, doppler_window) * current_data\n",
    "\n",
    "                # Apply 2D FFT and shift\n",
    "                rdm = np.fft.fft2(windowed_data)\n",
    "                rdm = np.fft.fftshift(rdm, axes=1)  # Shift along the Doppler axis (second axis in Python)\n",
    "\n",
    "                # Take the absolute value\n",
    "                rdm = np.abs(rdm)\n",
    "                \n",
    "                # Normalize the data before logarithmic scaling\n",
    "                rdm_max = np.max(rdm)\n",
    "                rdm_min = np.min(rdm)\n",
    "                rdm = (rdm - rdm_min) / (rdm_max - rdm_min + 1e-3)  # Avoid division by zero\n",
    "\n",
    "                # Slice the RDM to remove the mirrored part (keep only one half)\n",
    "                # Assuming the mirrored part is along the Range axis (axis 0)\n",
    "                half_index = rdm.shape[0] // 2\n",
    "                rdm = rdm[:half_index, :]\n",
    "\n",
    "                # Append to the list for the current channel\n",
    "                rdm_list.append(rdm)\n",
    "\n",
    "            # Append the result for the current channel\n",
    "            rdm_all_channels.append(rdm_list)\n",
    "\n",
    "        return np.array(rdm_all_channels)\n",
    "    \n",
    "    \n",
    "    def angle_of_arrival_processing(self, dataCube):\n",
    "        \"\"\"\n",
    "        Processes each frame in the dataCube for each channel to generate Angle of Arrival (AoA) heatmaps.\n",
    "\n",
    "        Args:\n",
    "            dataCube (np.ndarray): The raw data cubes to be processed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of processed AoA heatmaps for each channel.\n",
    "        \"\"\"\n",
    "        n_channels, n_frames, n_bins, n_elements = dataCube.shape\n",
    "        aoa_all_channels = []\n",
    "\n",
    "        # Define a window function for the spatial dimension (assuming ULA)\n",
    "        spatial_window = np.hanning(n_elements)\n",
    "\n",
    "        for channel_idx in range(n_channels):\n",
    "            aoa_list = []\n",
    "            for frame_idx in range(n_frames):\n",
    "                # Extract current data for the frame and channel\n",
    "                current_data = dataCube[channel_idx, frame_idx, :, :]\n",
    "\n",
    "                # Apply the window function to the antenna elements\n",
    "                windowed_data = current_data * spatial_window\n",
    "\n",
    "                # Apply 1D FFT along the spatial dimension (assuming antenna elements are along the last axis)\n",
    "                aoa_spectrum = np.fft.fft(windowed_data, axis=1)\n",
    "                aoa_spectrum = np.fft.fftshift(aoa_spectrum, axes=1)  # Shift to center the zero-frequency component\n",
    "\n",
    "                # Take the absolute value and normalize\n",
    "                aoa_spectrum = np.abs(aoa_spectrum)\n",
    "                aoa_spectrum_max = np.max(aoa_spectrum)\n",
    "                aoa_spectrum_min = np.min(aoa_spectrum)\n",
    "                aoa_normalized = (aoa_spectrum - aoa_spectrum_min) / (aoa_spectrum_max - aoa_spectrum_min + 1e-3)\n",
    "\n",
    "                # Append to the list for the current channel\n",
    "                aoa_list.append(aoa_normalized)\n",
    "\n",
    "            # Append the result for the current channel\n",
    "            aoa_all_channels.append(aoa_list)\n",
    "\n",
    "        return np.array(aoa_all_channels)\n",
    "    \n",
    "    \n",
    "    def generate_actuator_filter(self, dataCubes):\n",
    "        \"\"\"\n",
    "        Processes each frame in the dataCube for each channel to generate Range-Doppler Maps.\n",
    "\n",
    "        Args:\n",
    "            dataCube (np.ndarray): The raw data cubes to be processed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of processed Range-Doppler Maps for each channel.\n",
    "        \"\"\"\n",
    "        n_channels, n_frames, n_bins, n_doppler = dataCubes.shape\n",
    "        rdm_all_channels = []\n",
    "\n",
    "        # Define a window function for the range and Doppler dimensions\n",
    "        range_window = np.hanning(n_bins)\n",
    "        doppler_window = np.hanning(n_doppler)\n",
    "\n",
    "        for channel_idx in range(n_channels):\n",
    "            rdm_list = []\n",
    "            for frame_idx in range(n_frames):\n",
    "                # Extract current data for the frame and channel\n",
    "                current_data = dataCubes[channel_idx, frame_idx, :, :]\n",
    "\n",
    "                # Apply the Hanning window function\n",
    "                windowed_data = np.outer(range_window, doppler_window) * current_data\n",
    "\n",
    "                # Apply 2D FFT and shift\n",
    "                rdm = np.fft.fft2(windowed_data)\n",
    "                rdm = np.fft.fftshift(rdm, axes=1)  # Shift along the Doppler axis (second axis in Python)\n",
    "\n",
    "                # Take the absolute value\n",
    "                rdm = np.abs(rdm)\n",
    "                \n",
    "                # Normalize the data before logarithmic scaling\n",
    "                rdm_max = np.max(rdm)\n",
    "                rdm_min = np.min(rdm)\n",
    "                rdm = (rdm - rdm_min) / (rdm_max - rdm_min + 1e-3)  # Avoid division by zero\n",
    "\n",
    "                # Slice the RDM to remove the mirrored part (keep only one half)\n",
    "                # Assuming the mirrored part is along the Range axis (axis 0)\n",
    "                half_index = rdm.shape[0] // 2\n",
    "                rdm = rdm[:half_index, :]\n",
    "\n",
    "                # Append to the list for the current channel\n",
    "                rdm_list.append(rdm)\n",
    "\n",
    "            # Append the result for the current channel\n",
    "            rdm_all_channels.append(rdm_list)\n",
    "\n",
    "        return np.array(rdm_all_channels)[0,50:105,:,:]\n",
    "    \n",
    "    def manual_correlation(self, pattern, data, start_frame, end_frame):\n",
    "        pattern_normalized = (pattern - np.mean(pattern)) / np.std(pattern)\n",
    "        data_segment = data[start_frame:end_frame + 1]  # Plus 1 because the upper bound is exclusive\n",
    "        data_segment_normalized = (data_segment - np.mean(data_segment)) / np.std(data_segment)\n",
    "\n",
    "        correlation_score = np.sum(pattern_normalized * data_segment_normalized)\n",
    "\n",
    "        return correlation_score\n",
    "\n",
    "\n",
    "    def slide_pattern_over_data(self, pattern, data):\n",
    "        if data.ndim == 4:\n",
    "            data = data[0,:,:,:]\n",
    "        \n",
    "        # Normalize the pattern and data (z-scoring)\n",
    "        pattern_mean = np.mean(pattern)\n",
    "        pattern_std = np.std(pattern)\n",
    "        # Check if standard deviation is zero and handle it\n",
    "        pattern_normalized = (pattern - pattern_mean) / pattern_std if pattern_std else pattern - pattern_mean\n",
    "        \n",
    "        data_mean = np.mean(data, axis=(1, 2), keepdims=True)\n",
    "        data_std = np.std(data, axis=(1, 2), keepdims=True)\n",
    "        # Check if standard deviation is zero and handle it\n",
    "        data_normalized = (data - data_mean) / (data_std + 1e-8)  # Added a small constant to avoid division by zero\n",
    "\n",
    "        # Check for NaN or inf values\n",
    "        if np.isnan(data_normalized).any() or np.isinf(data_normalized).any():\n",
    "            raise ValueError(\"Data contains NaN or infinite values after normalization.\")\n",
    "\n",
    "        # Perform 3D correlation\n",
    "        match_scores = correlate(data_normalized, pattern_normalized, mode='valid', method='auto')\n",
    "        \n",
    "        # Sum over the spatial dimensions to get a single match score per frame position\n",
    "        match_scores_summed = match_scores.sum(axis=(1, 2))\n",
    "\n",
    "                \n",
    "        # Find peaks in the match scores\n",
    "        peaks, properties = find_peaks(match_scores_summed)\n",
    "\n",
    "        # Ensure 'peak_heights' is in properties, otherwise get the height of all peaks\n",
    "        if 'peak_heights' not in properties:\n",
    "            properties['peak_heights'] = match_scores_summed[peaks]\n",
    "\n",
    "        # Sort the peaks by their height\n",
    "        sorted_peaks = np.argsort(properties['peak_heights'])[::-1]\n",
    "\n",
    "        # Filter out peaks that are within 100 frames of each other\n",
    "        filtered_peaks = []\n",
    "        for peak_index in sorted_peaks:\n",
    "            peak = peaks[peak_index]\n",
    "            if not any(abs(peak - p) <= 100 for p in filtered_peaks):\n",
    "                filtered_peaks.append(peak)\n",
    "            if len(filtered_peaks) == 2:  # Stop when the top two valid peaks are found\n",
    "                break\n",
    "\n",
    "                \n",
    "        # Return the indices of the peaks as well as the match scores for plotting or further analysis\n",
    "        return filtered_peaks, match_scores_summed\n",
    "\n",
    "\n",
    "\n",
    "    def plot_match_scores(self, match_scores):\n",
    "        \"\"\"\n",
    "        Plots the match scores to help identify where the best matches are.\n",
    "\n",
    "        Args:\n",
    "            match_scores (np.ndarray): Array of match scores.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(match_scores, marker='o', linestyle='-')\n",
    "        plt.title('Pattern Match Score Across Frames')\n",
    "        plt.xlabel('Frame Position')\n",
    "        plt.ylabel('Match Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def create_gif(self, data, gif_filename, duration=0.038596):\n",
    "        \"\"\"\n",
    "        Creates a GIF from the provided data and saves it to the 'data/gifs' directory within the current working directory.\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): 3D or 4D array containing the image data.\n",
    "            gif_filename (str): Filename for the GIF, without path.\n",
    "            duration (float): Duration of each frame in the GIF.\n",
    "        \"\"\"\n",
    "        # Define the directory to save GIFs\n",
    "        gif_dir = os.path.join(os.getcwd(), 'data/gifs')\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(gif_dir, exist_ok=True)\n",
    "        # Full path for the GIF\n",
    "        gif_path = os.path.join(gif_dir, gif_filename)\n",
    "        \n",
    "        if data.ndim == 4:\n",
    "            data = data[0,:,:,:]\n",
    "\n",
    "        with imageio.get_writer(gif_path, mode='I', duration=duration) as writer:\n",
    "            if data.ndim == 3:  # If data is 3D, treat it as a sequence of 2D frames\n",
    "                for i in range(data.shape[0]):\n",
    "                    # Convert the data to an image (you might need to scale/normalize)\n",
    "                    frame = data[i, :, :].T\n",
    "                    plt.imshow(frame, cmap='gray')  # Use appropriate colormap\n",
    "                    plt.axis('off')  # Turn off axis\n",
    "                    plt.savefig('temp_frame.png', bbox_inches='tight', pad_inches=0)\n",
    "                    plt.close()\n",
    "                    writer.append_data(imageio.imread('temp_frame.png'))\n",
    "            # elif data.ndim == 4:  # If data is 4D, process each channel separately\n",
    "            #     for channel in range(data.shape[0]):\n",
    "            #         for i in range(data.shape[1]):\n",
    "            #             frame = data[channel, i, :, :].T\n",
    "            #             plt.imshow(frame, cmap='gray')  # Use appropriate colormap\n",
    "            #             plt.axis('off')  # Turn off axis\n",
    "            #             plt.savefig('temp_frame.png', bbox_inches='tight', pad_inches=0)\n",
    "            #             plt.close()\n",
    "            #             writer.append_data(imageio.imread('temp_frame.png'))\n",
    "\n",
    "        # Remove temporary frame image file\n",
    "        os.remove('temp_frame.png')\n",
    "\n",
    "        return gif_path  # Return the path where the GIF was saved\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('radartreepose_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "328376d6b0fabe9c025bc20907c001b430f3b746c3e3fb21cb53bd3449095683"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
