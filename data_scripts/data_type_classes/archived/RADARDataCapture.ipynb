{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a class for processing an FMCW RADAR data capture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from datetime import datetime\n",
    "from scipy.signal import find_peaks, correlate\n",
    "from scipy.ndimage import convolve\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMCWRADARDataCapture:\n",
    "    \"\"\"\n",
    "    Class for handling the capture, processing, and saving of FMCW RADAR data.\n",
    "\n",
    "    This class is designed to load Frequency-Modulated Continuous-Wave (FMCW) RADAR data from a specified HDF5 file,\n",
    "    process the data into a usable format, and save it as a NumPy file (either .npy or .npz format).\n",
    "\n",
    "    Attributes:\n",
    "        file_path (str): Path to the HDF5 file containing the RADAR data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initializes the FMCWRADARDataCapture class with the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the HDF5 file to be loaded and processed.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.output_path = self.file_path.replace(\"_Data\", \"_Data_NP\")\n",
    "        self.seconds_per_frame = 0.036352\n",
    "        normalized_actuator_pattern_in_radar_path = '/Users/danielcopeland/Library/Mobile Documents/com~apple~CloudDocs/MIT Masters/DRL/LABx/RADARTreePose/data/filter_patterns/normalized_actuator_pattern_in_radar.npy'\n",
    "        self.normalized_actuator_pattern_in_radar = np.load(normalized_actuator_pattern_in_radar_path)\n",
    "\n",
    "    def load_and_save(self, output_path=None, format='npy', save_npy = False):\n",
    "        if output_path is None:\n",
    "            output_path = self.output_path \n",
    "            output_path = os.path.splitext(output_path)[0]\n",
    "            \n",
    "        # Ensure the directory of the output path exists\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        dataCubes = np.zeros((4, 1000, 128, 256))\n",
    "        \n",
    "\n",
    "        # Open the HDF5 file for reading\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            # Extract parameters from the file\n",
    "            FreqStrt = file['/BrdCfg/FreqStrt'][()]\n",
    "            FreqStop = file['/BrdCfg/FreqStop'][()]\n",
    "            N = 256  # Number of samples per chirp, assuming it's given or extracted correctly\n",
    "            Np = 128  # Number of chirps per frame, assuming it's given or extracted correctly\n",
    "\n",
    "            # Calculate the effective bandwidth\n",
    "            B = (FreqStop - FreqStrt) / 284 * 256\n",
    "            \n",
    "            # Read the If signal\n",
    "            If = file['/If'][:]\n",
    "            NrFrms, Nx = If.shape\n",
    "            \n",
    "            # Initialize the dataCubes array to hold the processed data for each channel\n",
    "            dataCubes = np.zeros((4, NrFrms, N, Np))\n",
    "            \n",
    "            # Process each frame\n",
    "            for frame_idx in range(NrFrms-1):\n",
    "                # print(f'Processing RD Frame: {frame_idx + 1}')\n",
    "                for channel_idx in range(4):\n",
    "                    start_idx = channel_idx * N * Np\n",
    "                    end_idx = (channel_idx + 1) * N * Np\n",
    "                    # Correctly reshape and assign the data to the corresponding channel and frame\n",
    "                    reshaped_data = If[frame_idx, start_idx:end_idx].reshape(Np, N)\n",
    "                    dataCubes[channel_idx, frame_idx, :, :] = reshaped_data.transpose()  # Transpose operation\n",
    "\n",
    "            # # Verify by plotting the first channel of the first frame\n",
    "            # plt.imshow(dataCubes[0, 0, :, :])\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "        \n",
    "            if save_npy:\n",
    "                # Save data in the specified format\n",
    "                if format == 'npy':\n",
    "                    np.save(output_path, dataCubes)\n",
    "                elif format == 'npz':\n",
    "                    np.savez(output_path, dataCubes)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported format. Use 'npy' or 'npz'.\")\n",
    "            \n",
    "        return dataCubes\n",
    "\n",
    "    @staticmethod\n",
    "    def rawDataToDataCube(rawData, numFrames, numChirpsPerFrame, numSamplesPerChirp, numAntennas):\n",
    "        # Reshape and rearrange the rawData\n",
    "        matrixData = rawData.T.reshape(numChirpsPerFrame * numSamplesPerChirp, numFrames * numAntennas)\n",
    "        dataCubes = np.zeros((numFrames, numChirpsPerFrame, numSamplesPerChirp, numAntennas))\n",
    "\n",
    "        for frame in range(numFrames):\n",
    "            for antenna in range(numAntennas):\n",
    "                chirps = matrixData[:, frame * numAntennas + antenna]\n",
    "                chirpsMatrix = chirps.reshape(numSamplesPerChirp, numChirpsPerFrame)\n",
    "                dataCubes[frame, :, :, antenna] = chirpsMatrix.T\n",
    "                \n",
    "\n",
    "        return dataCubes.transpose((3,0,1,2))\n",
    "    \n",
    "    def range_doppler_processing(self, dataCubes):\n",
    "        \"\"\"\n",
    "        Processes each frame in the dataCube for each channel to generate Range-Doppler Maps.\n",
    "\n",
    "        Args:\n",
    "            dataCube (np.ndarray): The raw data cubes to be processed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of processed Range-Doppler Maps for each channel.\n",
    "        \"\"\"\n",
    "        n_channels, n_frames, n_bins, n_doppler = dataCubes.shape\n",
    "        rdm_all_channels = []\n",
    "\n",
    "        # Define a window function for the range and Doppler dimensions\n",
    "        range_window = np.hanning(n_bins)\n",
    "        doppler_window = np.hanning(n_doppler)\n",
    "\n",
    "        for channel_idx in range(n_channels):\n",
    "            rdm_list = []\n",
    "            for frame_idx in range(n_frames):\n",
    "                # Extract current data for the frame and channel\n",
    "                current_data = dataCubes[channel_idx, frame_idx, :, :]\n",
    "\n",
    "                # Apply the Hanning window function\n",
    "                windowed_data = np.outer(range_window, doppler_window) * current_data\n",
    "\n",
    "                # Apply 2D FFT and shift\n",
    "                rdm = np.fft.fft2(windowed_data)\n",
    "                rdm = np.fft.fftshift(rdm, axes=1)  # Shift along the Doppler axis (second axis in Python)\n",
    "\n",
    "                # Take the absolute value\n",
    "                rdm = np.abs(rdm)\n",
    "                \n",
    "                # Normalize the data before logarithmic scaling\n",
    "                rdm_max = np.max(rdm)\n",
    "                rdm_min = np.min(rdm)\n",
    "                rdm = (rdm - rdm_min) / (rdm_max - rdm_min + 1e-3)  # Avoid division by zero\n",
    "                \n",
    "                # Log scaling - apply log1p for numerical stability\n",
    "                rdm = np.log1p(rdm)\n",
    "\n",
    "                # Slice the RDM to remove the mirrored part (keep only one half)\n",
    "                # Assuming the mirrored part is along the Range axis (axis 0)\n",
    "                half_index = rdm.shape[0] // 2\n",
    "                rdm = rdm[:half_index, :]\n",
    "\n",
    "                # Append to the list for the current channel\n",
    "                rdm_list.append(rdm)\n",
    "\n",
    "            # Append the result for the current channel\n",
    "            rdm_all_channels.append(rdm_list)\n",
    "\n",
    "        return np.array(rdm_all_channels)\n",
    "    \n",
    "        \n",
    "    def range_doppler_processing_with_phase(self, dataCubes):\n",
    "        \"\"\"\n",
    "        Processes each frame in the dataCube for each channel to generate Range-Doppler Maps.\n",
    "\n",
    "        Args:\n",
    "            dataCube (np.ndarray): The raw data cubes to be processed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of processed Range-Doppler Maps for each channel, including unwrapped phase information.\n",
    "        \"\"\"\n",
    "        n_channels, n_frames, n_bins, n_doppler = dataCubes.shape\n",
    "        rdm_all_channels = []\n",
    "\n",
    "        # Define a window function for the range and Doppler dimensions\n",
    "        range_window = np.hanning(n_bins)\n",
    "        doppler_window = np.hanning(n_doppler)\n",
    "\n",
    "        for channel_idx in range(n_channels):\n",
    "            rdm_list = []\n",
    "            for frame_idx in range(n_frames):\n",
    "                # Extract current data for the frame and channel\n",
    "                current_data = dataCubes[channel_idx, frame_idx, :, :]\n",
    "\n",
    "                # Apply the Hanning window function\n",
    "                windowed_data = np.outer(range_window, doppler_window) * current_data\n",
    "\n",
    "                # Apply 2D FFT and shift\n",
    "                rdm_complex = np.fft.fft2(windowed_data)\n",
    "                rdm_complex = np.fft.fftshift(rdm_complex, axes=1)  # Shift along the Doppler axis\n",
    "\n",
    "                # Extract phase and apply phase unwrapping along the Doppler axis\n",
    "                phase_data = np.angle(rdm_complex)\n",
    "                unwrapped_phase = np.unwrap(phase_data, axis=1)  # Unwrapping along Doppler axis\n",
    "\n",
    "                # You could now use this unwrapped_phase for velocity calculations\n",
    "                # For demonstration, let's keep using the magnitude for the RDM visual representation\n",
    "                rdm = np.abs(rdm_complex)\n",
    "                \n",
    "                # Normalize the data before logarithmic scaling\n",
    "                rdm_max = np.max(rdm)\n",
    "                rdm_min = np.min(rdm)\n",
    "                rdm = (rdm - rdm_min) / (rdm_max - rdm_min + 1e-3)  # Avoid division by zero\n",
    "                \n",
    "                # Log scaling - apply log1p for numerical stability\n",
    "                rdm = np.log1p(rdm)\n",
    "\n",
    "                # Slice the RDM to remove the mirrored part (keep only one half)\n",
    "                half_index = rdm.shape[0] // 2\n",
    "                rdm = rdm[:half_index, :]\n",
    "\n",
    "                # Append to the list for the current channel\n",
    "                rdm_list.append(rdm)\n",
    "\n",
    "            # Append the result for the current channel\n",
    "            rdm_all_channels.append(rdm_list)\n",
    "\n",
    "        return np.array(rdm_all_channels)\n",
    "    \n",
    "    def angle_of_arrival_processing(self, dataCube):\n",
    "        \"\"\"\n",
    "        Processes each frame in the dataCube for each channel to generate Angle of Arrival (AoA) heatmaps.\n",
    "\n",
    "        Args:\n",
    "            dataCube (np.ndarray): The raw data cubes to be processed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of processed AoA heatmaps for each channel.\n",
    "        \"\"\"\n",
    "        n_channels, n_frames, n_bins, n_elements = dataCube.shape\n",
    "        aoa_all_channels = []\n",
    "\n",
    "        # Define a window function for the spatial dimension (assuming ULA)\n",
    "        spatial_window = np.hanning(n_elements)\n",
    "\n",
    "        for channel_idx in range(n_channels):\n",
    "            aoa_list = []\n",
    "            for frame_idx in range(n_frames):\n",
    "                # Extract current data for the frame and channel\n",
    "                current_data = dataCube[channel_idx, frame_idx, :, :]\n",
    "\n",
    "                # Apply the window function to the antenna elements\n",
    "                windowed_data = current_data * spatial_window\n",
    "\n",
    "                # Apply 1D FFT along the spatial dimension (assuming antenna elements are along the last axis)\n",
    "                aoa_spectrum = np.fft.fft(windowed_data, axis=1)\n",
    "                aoa_spectrum = np.fft.fftshift(aoa_spectrum, axes=1)  # Shift to center the zero-frequency component\n",
    "\n",
    "                # Take the absolute value and normalize\n",
    "                aoa_spectrum = np.abs(aoa_spectrum)\n",
    "                aoa_spectrum_max = np.max(aoa_spectrum)\n",
    "                aoa_spectrum_min = np.min(aoa_spectrum)\n",
    "                aoa_normalized = (aoa_spectrum - aoa_spectrum_min) / (aoa_spectrum_max - aoa_spectrum_min + 1e-3)\n",
    "\n",
    "                # Append to the list for the current channel\n",
    "                aoa_list.append(aoa_normalized)\n",
    "\n",
    "            # Append the result for the current channel\n",
    "            aoa_all_channels.append(aoa_list)\n",
    "\n",
    "        return np.array(aoa_all_channels)\n",
    "    \n",
    "    \n",
    "    def generate_actuator_filter(self, dataCubes):\n",
    "        \"\"\"\n",
    "        Processes each frame in the dataCube for each channel to generate Range-Doppler Maps.\n",
    "\n",
    "        Args:\n",
    "            dataCube (np.ndarray): The raw data cubes to be processed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of processed Range-Doppler Maps for each channel.\n",
    "        \"\"\"\n",
    "        n_channels, n_frames, n_bins, n_doppler = dataCubes.shape\n",
    "        rdm_all_channels = []\n",
    "\n",
    "        # Define a window function for the range and Doppler dimensions\n",
    "        range_window = np.hanning(n_bins)\n",
    "        doppler_window = np.hanning(n_doppler)\n",
    "\n",
    "        for channel_idx in range(n_channels):\n",
    "            rdm_list = []\n",
    "            for frame_idx in range(n_frames):\n",
    "                # Extract current data for the frame and channel\n",
    "                current_data = dataCubes[channel_idx, frame_idx, :, :]\n",
    "\n",
    "                # Apply the Hanning window function\n",
    "                windowed_data = np.outer(range_window, doppler_window) * current_data\n",
    "\n",
    "                # Apply 2D FFT and shift\n",
    "                rdm = np.fft.fft2(windowed_data)\n",
    "                rdm = np.fft.fftshift(rdm, axes=1)  # Shift along the Doppler axis (second axis in Python)\n",
    "\n",
    "                # Take the absolute value\n",
    "                rdm = np.abs(rdm)\n",
    "                \n",
    "                # Normalize the data before logarithmic scaling\n",
    "                rdm_max = np.max(rdm)\n",
    "                rdm_min = np.min(rdm)\n",
    "                rdm = (rdm - rdm_min) / (rdm_max - rdm_min + 1e-3)  # Avoid division by zero\n",
    "                \n",
    "                # Log scaling - apply log1p for numerical stability\n",
    "                rdm = np.log1p(rdm)\n",
    "\n",
    "                # Slice the RDM to remove the mirrored part (keep only one half)\n",
    "                # Assuming the mirrored part is along the Range axis (axis 0)\n",
    "                half_index = rdm.shape[0] // 2\n",
    "                rdm = rdm[:half_index, :]\n",
    "\n",
    "                # Append to the list for the current channel\n",
    "                rdm_list.append(rdm)\n",
    "\n",
    "            # Append the result for the current channel\n",
    "            rdm_all_channels.append(rdm_list)\n",
    "\n",
    "        return np.array(rdm_all_channels)[0,50:105,:,:]\n",
    "    \n",
    "    def manual_correlation(self, pattern, data, start_frame, end_frame):\n",
    "        pattern_normalized = (pattern - np.mean(pattern)) / np.std(pattern)\n",
    "        data_segment = data[start_frame:end_frame + 1]  # Plus 1 because the upper bound is exclusive\n",
    "        data_segment_normalized = (data_segment - np.mean(data_segment)) / np.std(data_segment)\n",
    "\n",
    "        correlation_score = np.sum(pattern_normalized * data_segment_normalized)\n",
    "\n",
    "        return correlation_score\n",
    "\n",
    "\n",
    "    def slide_pattern_over_data(self, pattern, data):\n",
    "        if data.ndim == 4:\n",
    "            data = data[0,:,:,:]\n",
    "        \n",
    "        # Normalize the pattern and data (z-scoring)\n",
    "        pattern_mean = np.mean(pattern)\n",
    "        pattern_std = np.std(pattern)\n",
    "        # Check if standard deviation is zero and handle it\n",
    "        pattern_normalized = (pattern - pattern_mean) / pattern_std if pattern_std else pattern - pattern_mean\n",
    "        \n",
    "        data_mean = np.mean(data, axis=(1, 2), keepdims=True)\n",
    "        data_std = np.std(data, axis=(1, 2), keepdims=True)\n",
    "        # Check if standard deviation is zero and handle it\n",
    "        data_normalized = (data - data_mean) / (data_std + 1e-8)  # Added a small constant to avoid division by zero\n",
    "        \n",
    "        # Check for NaN or inf values\n",
    "        if np.isnan(data_normalized).any() or np.isinf(data_normalized).any():\n",
    "            raise ValueError(\"Data contains NaN or infinite values after normalization.\")\n",
    "\n",
    "        # Perform 3D correlation\n",
    "        match_scores = correlate(data_normalized, pattern_normalized, mode='valid', method='auto')\n",
    "        \n",
    "        # Sum over the spatial dimensions to get a single match score per frame position\n",
    "        match_scores_summed = match_scores.sum(axis=(1, 2))\n",
    "\n",
    "                \n",
    "        # Find peaks in the match scores\n",
    "        peaks, properties = find_peaks(match_scores_summed)\n",
    "\n",
    "        # Ensure 'peak_heights' is in properties, otherwise get the height of all peaks\n",
    "        if 'peak_heights' not in properties:\n",
    "            properties['peak_heights'] = match_scores_summed[peaks]\n",
    "\n",
    "        # Sort the peaks by their height\n",
    "        sorted_peaks = np.argsort(properties['peak_heights'])[::-1]\n",
    "\n",
    "        # Filter out peaks that are within 100 frames of each other\n",
    "        filtered_peaks = []\n",
    "        for peak_index in sorted_peaks:\n",
    "            peak = peaks[peak_index]\n",
    "            if not any(abs(peak - p) <= 100 for p in filtered_peaks):\n",
    "                filtered_peaks.append(peak)\n",
    "            if len(filtered_peaks) == 2:  # Stop when the top two valid peaks are found\n",
    "                break\n",
    "\n",
    "                \n",
    "        # Return the indices of the peaks as well as the match scores for plotting or further analysis\n",
    "        return filtered_peaks, match_scores_summed\n",
    "    \n",
    "    def slide_normalized_actuator_pattern_over_data(self, data):\n",
    "        if data.ndim == 4:\n",
    "            data = data[0,:,:,:]\n",
    "\n",
    "        data_mean = np.mean(data, axis=(1, 2), keepdims=True)\n",
    "        data_std = np.std(data, axis=(1, 2), keepdims=True)\n",
    "        # Check if standard deviation is zero and handle it\n",
    "        data_normalized = (data - data_mean) / (data_std + 1e-8)  # Added a small constant to avoid division by zero\n",
    "        \n",
    "        # print(data_normalized.shape)\n",
    "        # print(self.normalized_actuator_pattern_in_radar.shape)\n",
    "\n",
    "        # Check for NaN or inf values\n",
    "        if np.isnan(data_normalized).any() or np.isinf(data_normalized).any():\n",
    "            raise ValueError(\"Data contains NaN or infinite values after normalization.\")\n",
    "\n",
    "        # Perform 3D correlation\n",
    "        match_scores = correlate(data_normalized, self.normalized_actuator_pattern_in_radar, mode='valid', method='auto')\n",
    "        \n",
    "        # Sum over the spatial dimensions to get a single match score per frame position\n",
    "        match_scores_summed = match_scores.sum(axis=(1, 2))\n",
    "\n",
    "                \n",
    "        # Find peaks in the match scores\n",
    "        peaks, properties = find_peaks(match_scores_summed)\n",
    "\n",
    "        # Ensure 'peak_heights' is in properties, otherwise get the height of all peaks\n",
    "        if 'peak_heights' not in properties:\n",
    "            properties['peak_heights'] = match_scores_summed[peaks]\n",
    "\n",
    "        # Sort the peaks by their height\n",
    "        sorted_peaks = np.argsort(properties['peak_heights'])[::-1]\n",
    "\n",
    "        # Filter out peaks that are within 100 frames of each other\n",
    "        filtered_peaks = []\n",
    "        for peak_index in sorted_peaks:\n",
    "            peak = peaks[peak_index]\n",
    "            if not any(abs(peak - p) <= 100 for p in filtered_peaks):\n",
    "                filtered_peaks.append(peak)\n",
    "            if len(filtered_peaks) == 2:  # Stop when the top two valid peaks are found\n",
    "                filtered_peaks.sort()\n",
    "                break\n",
    "\n",
    "                \n",
    "        # Return the indices of the peaks as well as the match scores for plotting or further analysis\n",
    "        return filtered_peaks, match_scores_summed\n",
    "\n",
    "\n",
    "\n",
    "    def plot_match_scores(self, match_scores):\n",
    "        \"\"\"\n",
    "        Plots the match scores to help identify where the best matches are.\n",
    "\n",
    "        Args:\n",
    "            match_scores (np.ndarray): Array of match scores.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(match_scores, marker='o', linestyle='-')\n",
    "        plt.title('Pattern Match Score Across Frames')\n",
    "        plt.xlabel('Frame Position')\n",
    "        plt.ylabel('Match Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # def create_gif(self, data, gif_filename):\n",
    "    #     \"\"\"\n",
    "    #     Creates a GIF from the provided data and saves it to the 'data/gifs' directory within the current working directory.\n",
    "\n",
    "    #     Args:\n",
    "    #         data (np.ndarray): 3D or 4D array containing the image data.\n",
    "    #         gif_filename (str): Filename for the GIF, without path.\n",
    "    #         duration (float): Duration of each frame in the GIF.\n",
    "    #     \"\"\"\n",
    "    #     # Define the directory to save GIFs\n",
    "    #     gif_dir = os.path.join(os.getcwd(), 'data/gifs')\n",
    "    #     # Create the directory if it doesn't exist\n",
    "    #     os.makedirs(gif_dir, exist_ok=True)\n",
    "    #     # Full path for the GIF\n",
    "    #     gif_path = os.path.join(gif_dir, gif_filename)\n",
    "        \n",
    "    #     if data.ndim == 4:\n",
    "    #         data = data[0,:,:,:]\n",
    "\n",
    "    #     with imageio.get_writer(gif_path, mode='I', duration=self.seconds_per_frame) as writer:\n",
    "    #         if data.ndim == 3:  # If data is 3D, treat it as a sequence of 2D frames\n",
    "    #             for i in range(data.shape[0]):\n",
    "    #                 # Convert the data to an image (you might need to scale/normalize)\n",
    "    #                 frame = data[i, :, :].T\n",
    "    #                 plt.imshow(frame, cmap='gray')  # Use appropriate colormap\n",
    "    #                 plt.axis('off')  # Turn off axis\n",
    "    #                 plt.savefig('temp_frame.png', bbox_inches='tight', pad_inches=0)\n",
    "    #                 plt.close()\n",
    "    #                 writer.append_data(imageio.imread('temp_frame.png'))\n",
    "    #         # elif data.ndim == 4:  # If data is 4D, process each channel separately\n",
    "    #         #     for channel in range(data.shape[0]):\n",
    "    #         #         for i in range(data.shape[1]):\n",
    "    #         #             frame = data[channel, i, :, :].T\n",
    "    #         #             plt.imshow(frame, cmap='gray')  # Use appropriate colormap\n",
    "    #         #             plt.axis('off')  # Turn off axis\n",
    "    #         #             plt.savefig('temp_frame.png', bbox_inches='tight', pad_inches=0)\n",
    "    #         #             plt.close()\n",
    "    #         #             writer.append_data(imageio.imread('temp_frame.png'))\n",
    "\n",
    "    #     # Remove temporary frame image file\n",
    "    #     os.remove('temp_frame.png')\n",
    "\n",
    "    #     return gif_path  # Return the path where the GIF was saved\n",
    "\n",
    "\n",
    "    # def create_gif(self, data, gif_filename, fp_data_capture):\n",
    "    #     # Define the directory to save GIFs\n",
    "    #     gif_dir = os.path.join(os.getcwd(), 'data/gifs')\n",
    "    #     # Create the directory if it doesn't exist\n",
    "    #     os.makedirs(gif_dir, exist_ok=True)\n",
    "    #     # Full path for the GIF\n",
    "    #     gif_path = os.path.join(gif_dir, gif_filename)\n",
    "        \n",
    "    #     if data.ndim == 4:\n",
    "    #         data = data[0,:,:,:]\n",
    "        \n",
    "    #     # Sort the lift and down frames\n",
    "    #     lift_frames = sorted(fp_data_capture.foot_lift_frames_after_actuator)\n",
    "    #     down_frames = sorted(fp_data_capture.foot_down_frames_after_actuator)\n",
    "\n",
    "    #     # Create an iterator for the down frames\n",
    "    #     down_frames_iter = iter(down_frames)\n",
    "    #     next_down_frame = next(down_frames_iter, None)\n",
    "\n",
    "    #     # Initialize the status as 'Foot Down' and prepare for the first 'Foot Up'\n",
    "    #     status = 'Foot Down'\n",
    "    #     next_lift_frame = lift_frames.pop(0) if lift_frames else None\n",
    "\n",
    "    #     # Iterate through each frame to create the GIF\n",
    "    #     for i, frame_data in enumerate(data):\n",
    "    #         frame = Image.fromarray(frame_data)\n",
    "    #         draw = ImageDraw.Draw(frame)\n",
    "\n",
    "    #         # Check if it's time to change the status\n",
    "    #         if next_lift_frame is not None and i >= next_lift_frame:\n",
    "    #             status = 'Foot Up'\n",
    "    #             next_down_frame = next(down_frames_iter, None)\n",
    "    #             next_lift_frame = lift_frames.pop(0) if lift_frames else None\n",
    "    #         elif next_down_frame is not None and i >= next_down_frame:\n",
    "    #             status = 'Foot Down'\n",
    "\n",
    "    #         # Annotate the frame with the current status\n",
    "    #         draw.text((10, 10), status, fill=\"white\")\n",
    "\n",
    "    #         if i == 0 or status == 'Foot Up':\n",
    "    #             # Save the first frame or 'Foot Up' frames with longer duration\n",
    "    #             writer.append_data(np.array(frame), duration=fp_data_capture.seconds_per_frame)\n",
    "    #         else:\n",
    "    #             writer.append_data(np.array(frame))\n",
    "\n",
    "    #     # Remove temporary frame image file\n",
    "    #     os.remove('temp_frame.png')\n",
    "\n",
    "    #     return gif_path  # Return the path where the GIF was saved\n",
    "    \n",
    "    \n",
    "    def create_gif(self, data, gif_filename, fp_data_capture):\n",
    "        \"\"\"\n",
    "        Creates a GIF from the provided data and saves it to the 'data/gifs' directory within the current working directory.\n",
    "        It also prints \"Foot Down\" initially on the GIF, then changes to \"Foot Up\" at the frame number for foot up,\n",
    "        then \"Foot Down\" at the next foot down, and so on based on the attributes \n",
    "        fp_data_capture.foot_lift_frames_after_actuator and fp_data_capture.foot_down_frames_after_actuator.\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): 3D array containing the image data.\n",
    "            gif_filename (str): Filename for the GIF, without path.\n",
    "            fp_data_capture (FPDataCapture): FPDataCapture object with foot lift and down frames attributes.\n",
    "        \"\"\"\n",
    "        # Define the directory to save GIFs\n",
    "        gif_dir = os.path.join(os.getcwd(), 'data/gifs')\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(gif_dir, exist_ok=True)\n",
    "        # Full path for the GIF\n",
    "        gif_path = os.path.join(gif_dir, gif_filename)\n",
    "        \n",
    "        if data.ndim == 4:\n",
    "            data = data[0,:,:,:]\n",
    "        \n",
    "        # Initialize the writer\n",
    "        with imageio.get_writer(gif_path, mode='I', duration=fp_data_capture.seconds_per_frame) as writer:\n",
    "            status = 'Foot Down'  # Initial status\n",
    "            for i in range(data.shape[0]):\n",
    "                frame = data[i, :, :].T  # Assuming data is 3D, with shape (frames, height, width)\n",
    "                        # Calculate zero velocity column and quarter distance\n",
    "                zero_vel = int(frame.shape[0] / 2)\n",
    "                quarter_distance = int(frame.shape[1] / 4)\n",
    "                \n",
    "                # Extracting the first quarter on the x-axis (after transpose)\n",
    "                first_quarter_frame = frame[:, :quarter_distance]\n",
    "                \n",
    "                # Calculate start and end indices for extracting the middle 23 values on the y-axis\n",
    "                mid_start = zero_vel - 11  # Middle minus half of 23 (to center the 23 values)\n",
    "                mid_end = zero_vel + 12  # Middle plus half of 23\n",
    "                \n",
    "                # Extracting the middle 23 values on the y-axis\n",
    "                middle_23_values_frame = first_quarter_frame[mid_start:mid_end, :]\n",
    "                plt.figure(figsize=(4, 4))\n",
    "                plt.imshow(middle_23_values_frame, cmap='gray')\n",
    "                plt.text(5, 5, status, color='white', fontsize=20)  # Position text at top-left\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Check and update the status based on frame number\n",
    "                if i in fp_data_capture.foot_lift_frames_after_actuator:\n",
    "                    status = 'Foot Up'\n",
    "                elif i in fp_data_capture.foot_down_frames_after_actuator:\n",
    "                    status = 'Foot Down'\n",
    "                \n",
    "                # Save the annotated frame to a temporary file\n",
    "                plt.savefig('temp_frame.png', bbox_inches='tight', pad_inches=0)\n",
    "                plt.close()\n",
    "                \n",
    "                # Append the frame to the GIF\n",
    "                writer.append_data(imageio.imread('temp_frame.png'))\n",
    "        \n",
    "        # Remove the temporary frame image file\n",
    "        os.remove('temp_frame.png')\n",
    "        \n",
    "        # Return the path where the GIF was saved\n",
    "        return gif_path\n",
    "\n",
    "    def process_and_save_channels_tx_separately(self, data, output_folder_path, file_name):\n",
    "        # Define transition times in seconds\n",
    "        transition_times = [\n",
    "            (8.5, 11.5),\n",
    "            (16.5, 19.5),\n",
    "            (24.5, 27.5)\n",
    "        ]\n",
    "        \n",
    "        # Calculate time per frame assuming 36 seconds for 1000 frames\n",
    "        time_per_frame = 36 / 1000\n",
    "        \n",
    "        # Calculate frame indices for each transition\n",
    "        transition_frames = [\n",
    "            (round(transition_time[0] / time_per_frame), round(transition_time[1] / time_per_frame))\n",
    "            for transition_time in transition_times\n",
    "        ]\n",
    "        \n",
    "        # Ensure the output folder exists\n",
    "        specific_output_folder_path = os.path.join(output_folder_path, file_name[:2])\n",
    "        if not os.path.exists(specific_output_folder_path):\n",
    "            os.makedirs(specific_output_folder_path)\n",
    "        \n",
    "        # Process and save each channel for each transition period\n",
    "        for channel_idx in range(data.shape[0]):\n",
    "            for i, (start_frame, end_frame) in enumerate(transition_frames):\n",
    "                # Initialize an empty list to collect processed frames for the current transition\n",
    "                processed_frames = []\n",
    "                \n",
    "                # Extract and process relevant frames for the current transition\n",
    "                for frame_idx in range(start_frame, end_frame):\n",
    "                    frame = data[channel_idx, frame_idx, :, :].T  # Transpose the frame\n",
    "                    \n",
    "                    # Calculate zero velocity column and quarter distance\n",
    "                    zero_vel = int(frame.shape[0] / 2)\n",
    "                    quarter_distance = int(frame.shape[1] / 4)\n",
    "                    \n",
    "                    # Extracting the first quarter on the x-axis (after transpose)\n",
    "                    first_quarter_frame = frame[:, :quarter_distance]\n",
    "                    \n",
    "                    # Calculate start and end indices for extracting the middle 23 values on the y-axis\n",
    "                    mid_start = zero_vel - 11\n",
    "                    mid_end = zero_vel + 12\n",
    "                    \n",
    "                    # Extract the middle 23 values on the y-axis from the first quarter\n",
    "                    processed_frame = first_quarter_frame[mid_start:mid_end, :]\n",
    "                    \n",
    "                    # Append the processed frame to the list\n",
    "                    processed_frames.append(processed_frame)\n",
    "                \n",
    "                # Convert the list of processed frames into a 3D numpy array\n",
    "                processed_frames_array = np.array(processed_frames)[:83,:,:]\n",
    "                \n",
    "                # Define the output file name for the processed data of the current transition\n",
    "                output_filename = os.path.join(\n",
    "                    specific_output_folder_path,\n",
    "                    f\"{file_name}_channel_{channel_idx+1}_tx{i+1}.npy\"\n",
    "                )\n",
    "                \n",
    "                # Save the 3D array to a binary file in NumPy .npy format\n",
    "                np.save(output_filename, processed_frames_array)\n",
    "        \n",
    "        print(f\"{file_name} processed and saved\")\n",
    "\n",
    "    def process_and_save_channels_separately(self, data, output_folder_path, file_name):\n",
    "\n",
    "            # Ensure the output folder exists\n",
    "            specific_output_folder_path = os.path.join(output_folder_path, file_name[:2])\n",
    "            if not os.path.exists(specific_output_folder_path):\n",
    "                os.makedirs(specific_output_folder_path)\n",
    "            \n",
    "            # Process and save each channel for each transition period\n",
    "            for channel_idx in range(data.shape[0]):\n",
    "                \n",
    "                # Initialize an empty list to collect processed frames for the current transition\n",
    "                processed_frames = []\n",
    "                \n",
    "                # Extract and process relevant frames for the current transition\n",
    "                for frame_idx in range(data.shape[1]):\n",
    "                    frame = data[channel_idx, frame_idx, :, :].T  # Transpose the frame\n",
    "                    \n",
    "                    # Calculate zero velocity column and quarter distance\n",
    "                    zero_vel = int(frame.shape[0] / 2)\n",
    "                    quarter_distance = int(frame.shape[1] / 4)\n",
    "                    \n",
    "                    # Extracting the first quarter on the x-axis (after transpose)\n",
    "                    first_quarter_frame = frame[:, :quarter_distance]\n",
    "                    \n",
    "                    # Calculate start and end indices for extracting the middle 23 values on the y-axis\n",
    "                    mid_start = zero_vel - 11\n",
    "                    mid_end = zero_vel + 12\n",
    "                    \n",
    "                    # Extract the middle 23 values on the y-axis from the first quarter\n",
    "                    processed_frame = first_quarter_frame[mid_start:mid_end, :]\n",
    "                    \n",
    "                    # Append the processed frame to the list\n",
    "                    processed_frames.append(processed_frame)\n",
    "                \n",
    "                # Convert the list of processed frames into a 3D numpy array\n",
    "                processed_frames_array = np.array(processed_frames)\n",
    "                \n",
    "                # Define the output file name for the processed data of the current transition\n",
    "                output_filename = os.path.join(\n",
    "                    specific_output_folder_path,\n",
    "                    f\"{file_name}_channel_{channel_idx+1}.npy\"\n",
    "                )\n",
    "                \n",
    "                # Save the 3D array to a binary file in NumPy .npy format\n",
    "                np.save(output_filename, processed_frames_array)\n",
    "            \n",
    "            print(f\"{file_name} processed and saved\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('radartreepose_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "328376d6b0fabe9c025bc20907c001b430f3b746c3e3fb21cb53bd3449095683"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
